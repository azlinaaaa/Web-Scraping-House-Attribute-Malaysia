import csv
import time
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

# Setup WebDriver options
options = Options()
# options.add_argument("--headless")  # Uncomment if you want to run without opening the browser
options.add_argument("--disable-gpu")
options.add_argument("--no-sandbox")
options.add_argument(
    "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
)

# Initialize WebDriver
driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

# Open iProperty page
url = "https://www.iproperty.com.my/sale/sabah-cc02j/all-residential/?l1"
driver.get(url)

houses = []  # Store all houses from all pages

try:
    while True:  # Continue scraping until no more pages
        # Wait until listings appear
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.XPATH, "//ul[contains(@class, 'listing-list')]/li"))
        )

        time.sleep(3)  # Allow time for full data load

        # Scroll to load more data
        scroll_pause_time = 2
        last_height = driver.execute_script("return document.body.scrollHeight")

        while True:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(scroll_pause_time)
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                break
            last_height = new_height

        # Find all listings
        listings = driver.find_elements(By.XPATH, "//ul[contains(@class, 'listing-list')]/li")

        for listing in listings:
            housename = listing.find_elements(By.XPATH, ".//h2[contains(@class, 'PremiumCardstyle__TitleWrapper')]")
            housename = housename[0].text.strip() if housename else "N/A"

            price = listing.find_elements(By.XPATH, ".//li[contains(@class, 'ListingPricestyle__ItemWrapper')]")
            price = price[0].text.strip() if price else "N/A"

            location = listing.find_elements(By.XPATH, ".//div[contains(@class, 'PremiumCardstyle__AddressWrapper')]")
            location = location[0].text.strip() if location else "N/A"

            pricePerSqft = listing.find_elements(By.XPATH,
                                                 ".//div[contains(@class, 'ListingPricestyle__PricePSFWrapper')]")
            pricePerSqft = pricePerSqft[0].text.strip() if pricePerSqft else "N/A"

            description = listing.find_elements(By.XPATH, ".//div[contains(@class, 'attributes-description')]")
            description = description[0].text.strip() if description else "N/A"

            bedroom = listing.find_elements(By.XPATH, ".//li[contains(@class, 'bedroom-facility')]")
            bedroom = bedroom[0].text.strip() if bedroom else "N/A"

            bathroom = listing.find_elements(By.XPATH, ".//li[contains(@class, 'bathroom-facility')]")
            bathroom = bathroom[0].text.strip() if bathroom else "N/A"

            carpark = listing.find_elements(By.XPATH, ".//li[contains(@class, 'carPark-facility')]")
            carpark = carpark[0].text.strip() if carpark else "N/A"

            agentname = listing.find_elements(By.XPATH, ".//div[contains(@class, 'ListingHeadingstyle__HeadingTitle')]")
            agentname = agentname[0].text.strip() if agentname else "N/A"

            postdate = listing.find_elements(By.XPATH,
                                             ".//p[contains(@class, 'ListingHeadingstyle__HeadingCreationDate')]")
            postdate = postdate[0].text.strip() if postdate else "N/A"

            houses.append([housename, price, location, pricePerSqft, description, bedroom, bathroom, carpark, agentname,
                           postdate])

        # Find "Next" button
        try:
            next_button = driver.find_element(By.XPATH, "//a[contains(@aria-label, 'Go to next page')]")
            if "disabled" in next_button.get_attribute("class"):
                break  # Stop if button is disabled
            next_button.click()  # Click to go to next page
            time.sleep(3)  # Wait before scraping next page
        except:
            break  # Exit loop if no "Next" button is found

    # Save data to CSV
    csv_filename = "iproperty_dataset_sabah.csv"
    with open(csv_filename, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(
            ["House Name", "Price", "Location", "Price per SqFt", "Description", "Bedroom", "Bathroom", "Carpark",
             "Agent Name", "Post Date"])
        writer.writerows(houses)

    print(f"Data from all pages successfully saved in {csv_filename} ({len(houses)} houses).")

except Exception as e:
    print("Error:", e)

finally:
    driver.quit()  # Close browser

